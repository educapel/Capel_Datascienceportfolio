# -*- coding: utf-8 -*-
"""Carspriceprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IM511UNcZOZ_qtoECj-_EfLJ4LzRZ1tM

# **Predicting BMW prices**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno

df= pd.read_csv('/content/drive/MyDrive/final project master/project1/bmw_pricing_v2.csv')

"""Let's see the variables in the data frame and some more information of our dataframe"""

df.head()

"""# **EDA**"""

plt.figure(figsize=(20,10))
sns.countplot(y=df.tipo_coche, data=df)
plt.title('Number of cars based on size', fontsize=20)

"""As could be seen, most of the car observation are estate cars, something aligned with reality since people prefers comfortability and spacious over style like coupe."""

for i in 'fecha_registro', 'fecha_venta':
  df[i]=pd.to_datetime(df[i])
df.info()

plt.figure(figsize=(20,10))
sns.countplot(y= df.fecha_registro.dt.year)
plt.title('Number of cars in different years', fontsize=20)

"""As it could be seen in the plot, the data is not relatively new since most of the price observation goes from 2011 to 2015. Meaning something not good because our prediction would be based on past data."""

plt.figure(figsize=(20,10))
df.groupby("tipo_gasolina")['precio'].mean().plot(kind='bar', color='green')
plt.title('Average price based on engine type', fontsize=20)

"""As the plot shows, there is a misclassification in the type of fuel engine, although it can be seen how the hybrid_petrol engine is the most expensive on average price.

No more exploratory data analysis in general can be conducted since there are misclassifications within categorical variables. It would be needed some data cleaning, but first let´s end checking the data structure.

# **1 Data structure/ distribution**
"""

df.shape

df.info()

df.describe(percentiles=[0.1,0.25,0.5, 0.75])

"""Checking basic statistics we can see some mismatch between the percentile 10% and the min prices of bmws, meaning the possibility of outliers in the prices, which is our target for predictions.

**2 NaN values and duplicates**
"""

# Let's start with Na values and duplicates.

df.isna().sum()

df[df.duplicated()]

df.isna().sum()

msno.matrix(df, color = (0.5, 0.5, 0.5))

df[df['color'].isna()]

df[df['tipo_coche'].isna()]

df1=df.copy()

"""NaN values will be deleted since they are not significant in each column, this can be observer in the msno.matrix."""

df1.dropna(inplace=True)
df1.isna().sum()

df.shape

df1.shape

"""There have been 51 observations deleted, something good.
Before splitting variables into categorical, booleans and numerics we will do the univariable analysis to see if there has to be committed any additional data cleaning.

3.   **Univariable analysis**
"""

# Univariable analysis, starting with categorical values, which represent the mayority of variables.
for i in df1.columns:
  if df1.columns.dtype.kind == 'O':
    print('\n', i, df1[i].value_counts(), '\n')

"""At first glance, we can see that brand columns make no sense to keep. In addition, there seems to be a misclassification in tipo_gasolina ('tipo_gasolina diesel'), mixing diesel and gasoline. Furthermore, there seems to be other irrelevant variables to predict the prices like, asientos_traseros_plegables,  volante_regulable, bluetooth,  alerta_lim_velocidad due to their real value in the car and the value that represent for the car, we will check it in the correlation later on.     """

#Numercial values analysis(no presence of booleans)
for i in df.columns:
  if df[i].dtype.kind in ('f', 'i'):
    print(df.hist(i))

"""All the numericas variables present a positive skewness, where it has to be highlighted the presence of outlier in price and the possibility of some in potency, there are no usual cars with less than 100 house power.

Going back to categorical variable, let's start the data cleaning with model column since there seems to be to many models.

*Data cleaning*
"""

# Model has no impact in the prediction

df1.drop('marca', axis=1, inplace=True)

"""We've seen some mismatches in modelo, and few observations in a specific type of tipo_gasolina and color. Hence, we will proceed to delete them in order to clean the dataset, defining a function which works as: Creating an empty list, counting the dataset values, and resetting the index to make it a dataset. 

Then the data set is traversed and it is asked if the new dataset that has the counts of the values ​​is less than the threshold, if so, it is added to a list.
"""

df2=df1.copy()

df2.modelo

for i in df.modelo:
  print(i)

def classify_series(x):
    if str(x).startswith('1') or str(x).startswith('X1') or str(x).startswith('M1'):
        return 'serie1'
    elif str(x).startswith('2') or str(x).startswith('X2') or str(x).startswith('M2'):
        return 'serie2'
    elif str(x).startswith('3') or str(x).startswith('X3') or str(x).startswith('M3'):
        return 'serie3'
    elif str(x).startswith('4') or str(x).startswith('X4') or str(x).startswith('Z4') or str(x).startswith('M4'):
        return 'serie4'
    elif str(x).startswith('5') or str(x).startswith('X5') or str(x).startswith('M5'):
        return 'serie5'
    elif str(x).startswith('6') or str(x).startswith('X6'):
        return 'serie6'
    elif str(x).startswith('7'):
      return 'serie7'
    else:
        return 'hybrid'


# Creating a new column 'series' based on the values in 'modelo'
df2['series'] = df2['modelo'].apply(classify_series)

pd.reset_option('max_rows', None)
df2.series.value_counts()

"""In the new column called 'series' it has been grouped the old observation from modelo by the type of series of each car's observation, clearly seen in the function defined. So the column modelo is not needed anymore. 
On the other hand as we saw before there was another misclassification in the type of fuel in 'typo_gasolina'

"""

df2['tipo_gasolina'].value_counts()

df2['tipo_gasolina'] = df2['tipo_gasolina'].apply(lambda x: x.lower())

plt.figure(figsize=(5,5))
sns.countplot(x="tipo_gasolina", data=df2)

"""As could be seen almost every car in the data frame use diesel fuel. This is due to fact that almost any of the cars where M1, M2, M3... which are the ones that uses petrol. The quantity of this cars observation was seen where it was made the transformation from 'modelo' to 'series'

One we got the data set apparently clean, we're going to check is every has worked correctly.
"""

for i in df2.columns:
  if df2[i].dtype.kind in ('O'):
    print('\n', i , df2[i].value_counts(), '\n')

"""It's been forgotten to delete the 'modelo' columns"""

df2.drop('modelo', axis=1, inplace=True)

df2

"""For future preprocessing steps, it'd be easy to delete fechas(dates)"""

for i in [ 'fecha_venta', 'fecha_registro']:
  del(df2[i])

df2

"""#  **4 Spliting variables into categorical, numericals, booleans and target**

This is done for future prepocesing steps such as getting dummies variables(it could be done later on).
"""

df2.info()

target=['precio']
def get_variables(dataset):
  
  numeric_list=[]
  categorical_list=[]
  boolean_list=[]

  for i in dataset:
        if    (dataset[i].dtype.kind in ("f","i")) and len(dataset[i].unique())!= 2  and i not in target:
              numeric_list.append(i)
        elif  (dataset[i].dtype.kind in('O', 'b'))  and len(dataset[i].unique())== 2  and i not in target:
              boolean_list.append(i)
        elif  (dataset[i].dtype.kind == "O")  and i not in target:
              categorical_list.append(i)
                
  return numeric_list, categorical_list, boolean_list

numeric_list, categorical_list, boolean_list = get_variables(df2)

boolean_list

df2

"""Now we're going to continue with the preprocessing part, where we are going to see correlation between variables, how the target variable behaves and we will see the relation between target and independent variables.

# **5 Preprocesing**

**5.1Correlation**
"""

df_prep= df2.copy()

df_prep.info()

df_prep

corr= df_prep.corr()

corr.style.background_gradient(cmap = 'coolwarm')

"""There is no strong multicollinearity between independent variable, but it's can be seen how gps does have little impact on price and how horsepower will influence the price. Future correlation will be observed when categorical and booleans variables will be converted into numeric ones.

**5.2 Analysis of the target variable**

Let's continue by analysis the target variable, which seems to have outlier, this was seen by basic statistics at the beggining
"""

df_prep.precio.describe(percentiles = [0.1, 0.9])

sns.displot(x='precio', data= df_prep, bins=20)

"""In the plot it's can be seen how the price stars by 0, something that make no sence. Also we can observe how the price distributio has a positive skewness and positive kurtosis."""

sns.boxplot( x= df_prep.precio)

df_prep[df_prep['precio'] < 500]

df_prep['precio'] = np.where(df_prep['precio'] < 500, df_prep['precio'].mean(), df_prep['precio'])

"""If you check the bmws market on the internet, it's clearly seen, at least in the Spanish market, that there's any bmws for less than 1000 eu, no matter his age or the km. So, let's change these observations to the mean.

On the other hand, as it's seen in the box plot the price seems to have outliers in the higher values.
"""

df_prep[df_prep['precio'] > 90000]

"""There is a clear mistake in the price for the last two observations, so let's do the same with higher values."""

df_prep['precio'] = np.where(df_prep['precio'] > 100000, df_prep['precio'].mean(), df_prep['precio'])

"""In order to change the price distibrution, we're going to create a new target variable through applying log10."""

df_prep['log_precio'] = np.log10(df_prep['precio'])

sns.boxplot( x= df_prep['log_precio'])

sns.displot(x='log_precio', data= df_prep, bins=20)

"""**5.3 Target vs independent variables**

Applying a log transformation to the price data reduce the skewness and kurtosis. This works especially well for distributions with long right tails

Now we the target variable againts independent variables are going to be compared through violinplots in order to see their relevance in the target variable.
"""

df_prep

for i in categorical_list:
  plt.figure(figsize= (5,5))
  sns.violinplot(x= i, y= 'precio', data= df_prep)

"""In an overall way there are some characteristics of violinplot that has to be applied to this categorical variables. 
- If the violin plot does not have symmetry this means that the categorical variable doesn't follow a normal distribution and there is skewness.
-The wider the plot, the more spread out the data is.
-The taller the plot, the more data points there are for that level of the independent variable.
-If the plots are significantly different in shape or height, it may indicate that the target variable is affected by the independent variable
"""

for i in numeric_list:
  plt.figure(figsize = (5,5))
  sns.scatterplot(y= 'precio', x= i, data = df_prep)

"""As can be seen fecha_registro and potencia present a positive relation againts precio. On the other hand, km has negative relation versus price .

**5.4 Normalizing categorical variables**

Since the categorical variables don't follow a normal distribution, it's needed to normalize them, which is less sensitive to outliers than standardization. But first of all, let's get the dummy variables for our models and see the correlation to drop correlated columns
"""

df_prep_cat= pd.get_dummies( data= df_prep, columns= categorical_list)
df_prep_cat

corr = df_prep_cat.corr()
corr.style.background_gradient( cmap = 'coolwarm')

"""As could be seen the data set doesn't have any correlation from more than |0.8|, except 'tipo_gasolina_diesel' and 'tipo_gasolina_petrol', so it's don't needed to deleted any column, let's continue by normalizing the numerical values and dropping the log column since log column follows a normal distribution and this process if for variables which does not follow a normal distribution."""

del(df_prep_cat['tipo_gasolina_petrol'])

del(df_prep_cat['log_precio'])

from sklearn.preprocessing import MinMaxScaler

def minmaxtransformacion(variable):
  MinMaxResultado= MinMaxScaler()
  df_prep[variable] = MinMaxResultado.fit_transform(df_prep[variable].values.reshape(-1,1))
  return MinMaxResultado

for i in numeric_list:
  globals()[f'Minmax{i}']= minmaxtransformacion(i)

df_prep_cat.columns

"""Once we already have the numeric and categorical variables transformed in order to use the data for the model, it's also needed to give numerical values to boolean variables, if var[i]=True means 1 and vice-versa. This also could be done by getting dummy variables although drop_first would have to be True to avoid perfect multicollinearity with booleans variables or just by converting booleans variables into integers.."""

for i in boolean_list:
  df_prep_cat[i]=  df_prep_cat[i].astype('int')

pd.set_option('max_columns', None)
 df_prep_cat

pd.reset_option('max_columns')

corr= df_prep_cat.corr()
corr.style.background_gradient(cmap='coolwarm')

"""Now, our data is ready to use it in the models, since there is not multicollinearity higher than |0.8|.

# **Predicting models**

**1.Linear Regression**
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error

"""After the data preprocessing, we proceed to define the independent and depedent variables."""

X = df_prep_cat.drop( columns = 'precio', axis=1)
y = df_prep_cat['precio']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 100)

"""Now the data is splitted into training and test data. The train data is used to train the model under considerations."""

model1 = LinearRegression()
model1.fit(X_train, y_train)
y_predict1 = model1.predict( X_test)

"""We will first start with a very simple model such as Linear Regression and slowly increase the complexity and how this accounts for an increase or decrease in the mean squared error"""

y_predict1 = pd.DataFrame(y_predict1, columns= ['Predicted_Output'])

"""Now the test set is use as input to make predictions, and we calculate the mean_erros to see the model's performance so that they will be plot all together to make a comparaison of the model's performance"""

results = pd.concat([y_predict1, y_test.to_frame().reset_index(drop = True)], axis = 1, ignore_index = False)

"""In order to plot the actual vs the predicted output it's needed to create a dataframe with the predicted and acutal values, this is also a way to see the model's performance. If the actual values get closer to the predicted ones, the model is doing well."""

plt.figure(figsize=(10,10))
sns.regplot(data= results, y= 'Predicted_Output', x='precio', color='red')
plt.title('Comparaison between actual and predicted values', fontsize=20)

"""As we can see linear regression model is not doing very well since there is a big scatter in the output plot, it means that our model is not doing well on the test set."""

mse_linear = mean_squared_error(y_predict1, y_test)
mae_linear = mean_absolute_error(y_predict1, y_test)

"""**2. Support Vector Regressor**"""

from sklearn.svm import SVR

model2 = SVR()
model2.fit(X_train, y_train)
y_predict2 = model2.predict(X_test)

y_predict2 = pd.DataFrame( y_predict2, columns= ['Predicted_output_svr'])

result2 = pd.concat([y_predict2, y_test.to_frame().reset_index(drop= True)], axis=1, ignore_index= False)

plt.figure(figsize= (10,10))
sns.regplot(data = results, x='precio', y= 'Predicted_output_svr', color='red')
plt.title("Comparision of predicted values and the actual values", fontsize = 20)

mse_svr = mean_squared_error(y_predict2, y_test)
mae_svr = mean_absolute_error(y_predict2, y_test)

"""This plot clearly indicates that the model is not performing better than the Linear Regression model as the points between the prediction and the actual values are quite scattered from the line

**3. K- Neighbors Regressor**
"""

from sklearn.neighbors import KNeighborsRegressor

model3= KNeighborsRegressor()
model3.fit(X_train, y_train)
y_predict3 = model3.predict( X_test)

y_predict3 = pd.DataFrame( y_predict3, columns= ['predicted_output_KNR'])
result3 = pd.concat([y_predict3, y_test.to_frame().reset_index(drop=True)], axis= 1, ignore_index=False)

h

mse_knr = mean_squared_error(y_predict3, y_test)
mae_knr = mean_absolute_error(y_predict3, y_test)

"""As we can see K-neighbors Regression doesn't work very well since the actual values doesn't rely on the line prediction.

**4. Decision tree regression**
"""

from sklearn.tree import DecisionTreeRegressor

model4 = DecisionTreeRegressor()
model4.fit( X_train, y_train)
y_predict4 = model4.predict(X_test)

y_predict4 = pd.DataFrame( y_predict4, columns=[ 'predicted_output_dtr'])
result4= pd.concat([y_predict4, y_test.to_frame().reset_index(drop= True)], axis=1, ignore_index=False)

plt.figure(figsize=( 10, 10))
sns.regplot( data = result4, x= 'precio', y='predicted_output_dtr', color='blue')
plt.title('Comparision of predicted values and the actual values', fontsize = 20)

"""Decision tree regressor seems to work better than the two prevous model although so far, Linear regressor is the one which is working better."""

mse_dtr = mean_squared_error(y_predict4, y_test)
mae_dtr = mean_absolute_error(y_predict4, y_test)

"""**5. Gradient boost regressor**"""

from sklearn.ensemble import GradientBoostingRegressor

model5 = GradientBoostingRegressor()
model5.fit(X_train, y_train)
y_predict5= model5.predict(X_test)

y_predict5 = pd.DataFrame( y_predict5, columns= ['predicted_output_gbr'])
result5 = pd.concat([y_predict5, y_test.to_frame().reset_index(drop=False)], axis=1, ignore_index=False)

plt.figure(figsize=( 10, 10))
sns.regplot( data = result5, x= 'precio', y='predicted_output_gbr', color='red')
plt.title('Comparision of predicted values and the actual values', fontsize = 20)

mse_gbd = mean_squared_error(y_predict5, y_test)
mae_gbd = mean_absolute_error(y_predict5, y_test)

"""It's seems that gradient boosting regressor works better than the prevoius models, noting that the time complexity of this model is also high compared to the rest.
On the other hand, the mean absolute error (MAE) and mean squared error (MSE) are two common ways to calculate the average error. Both measures are used to evaluate the performance of regression models, which predict a continuous value. So let's plot them.
"""

data= { 'mean_squeared_error': [mse_linear,  mse_svr, mse_dtr, mse_knr, mse_gbd], 'mean_abs_error':[mae_linear, mae_svr, mae_dtr, mae_knr, mae_gbd] }
df_1= pd.DataFrame(data, index= ['linear_reg', 'suport_vector', 'decision_tree','K-neighbors', 'gradient_boosting' ])

df_1

df_1.plot.bar( y= 'mean_squeared_error')
plt.title("Barplot of various machine learning regression models with mean absolute error")

df_1.plot.bar( y= 'mean_abs_error')
plt.title("Barplot of various machine learning regression models with mean absolute error")

"""Based on the different model's outcomes it's clear that gradient boosting is the one working better followed by linear regression. Suport vector regression and K-neighbors should be not taking into consideration.However, the same model might perform the best for other tasks. Therefore, we should explore many models before deploying them in real-time.

On the other hand, we could increase the accuracy of our model in multiples ways by: 

-Using regularization (L1, L2 or L1+L2).              

-Using cross-validation

-Tuning hyperparameters

-Or using more ensembles, we already use gradient boosting regressor (which is an ensemble created from decision trees added sequentially to the model).
"""